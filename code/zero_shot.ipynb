{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import dgl\n",
    "import torch\n",
    "import pickle\n",
    "import networkx as nx\n",
    "from dgl.data import DGLDataset\n",
    "import torchvision.transforms as T\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "# model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img_path, texts, threshold):\n",
    "    image = Image.open(img_path)\n",
    "\n",
    "    inputs = processor(text=texts, images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Target image sizes (height, width) to rescale box predictions [batch_size, 2]\n",
    "    target_sizes = torch.Tensor([image.size[::-1]])\n",
    "    # Convert outputs (bounding boxes and class logits) to COCO API\n",
    "    results = processor.post_process_object_detection(outputs=outputs, threshold=threshold, target_sizes=target_sizes)\n",
    "\n",
    "    i = 0  # Retrieve predictions for the first image for the corresponding text queries\n",
    "    text = texts[i]\n",
    "    boxes, scores, labels = results[i][\"boxes\"].cpu().detach().numpy(), results[i][\"scores\"], results[i][\"labels\"]\n",
    "   \n",
    "    # Print detected objects and rescaled box coordinates\n",
    "    for box, score, label in zip(boxes, scores, labels):\n",
    "        box = [round(i, 2) for i in box.tolist()]\n",
    "        print(f\"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}\")\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    ax.imshow(image)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    for score, box, label in zip(scores, boxes, labels):\n",
    "        # if score < score_threshold:\n",
    "        #     continue\n",
    "        x, y, w, h = box\n",
    "        rect = plt.Rectangle((x, y), w, h, fill=False, \n",
    "                             edgecolor='red', \n",
    "                             linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x, y - 2, \n",
    "                f\"{text[label]}: {score:.2f}\", \n",
    "                color='red', \n",
    "                fontsize=12, \n",
    "                ha='left', \n",
    "                va='top')\n",
    "    plt.show()\n",
    "    # return boxes, scores, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.image_utils import ImageFeatureExtractionMixin\n",
    "mixin = ImageFeatureExtractionMixin()\n",
    "\n",
    "def predictions(image, text_queries):\n",
    "    inputs = processor(text=text_queries, images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    # Load example image\n",
    "    image_size = model.config.vision_config.image_size\n",
    "    image = mixin.resize(image, image_size)\n",
    "    input_image = np.asarray(image).astype(np.float32) / 255.0\n",
    "\n",
    "    # Threshold to eliminate low probability predictions\n",
    "    score_threshold = 0.1\n",
    "\n",
    "    # Get prediction logits\n",
    "    logits = torch.max(outputs[\"logits\"][0], dim=-1)\n",
    "    scores = torch.sigmoid(logits.values).cpu().detach().numpy()\n",
    "\n",
    "    # Get prediction labels and boundary boxes\n",
    "    labels = logits.indices.cpu().detach().numpy()\n",
    "    boxes = outputs[\"pred_boxes\"][0].cpu().detach().numpy()\n",
    "\n",
    "    # # 绘制图\n",
    "    # fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    # ax.imshow(input_image, extent=(0, 1, 1, 0))\n",
    "    # ax.set_axis_off()\n",
    "\n",
    "    boxes_node = []\n",
    "    labels_node = []\n",
    "\n",
    "    for score, box, label in zip(scores, boxes, labels):\n",
    "      if score < score_threshold:\n",
    "        continue\n",
    "      boxes_node.append(box)\n",
    "      labels_node.append(label)\n",
    "      # cx, cy, w, h = box\n",
    "      \n",
    "      # ax.plot([cx-w/2, cx+w/2, cx+w/2, cx-w/2, cx-w/2],\n",
    "      #         [cy-h/2, cy-h/2, cy+h/2, cy+h/2, cy-h/2], \"r\")\n",
    "      # ax.text(\n",
    "      #     cx - w / 2,\n",
    "      #     cy + h / 2 + 0.015,\n",
    "      #     f\"{text_queries[label]}: {score:1.2f}\",\n",
    "      #     ha=\"left\",\n",
    "      #     va=\"top\",\n",
    "      #     color=\"red\",\n",
    "      #     bbox={\n",
    "      #         \"facecolor\": \"white\",\n",
    "      #         \"edgecolor\": \"red\",\n",
    "      #         \"boxstyle\": \"square,pad=.3\"\n",
    "      #     })\n",
    "    return boxes_node,labels_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [[\n",
    "        \"a photo of people\",\n",
    "        \"a photo of building\",\n",
    "        \"a photo of sky\",\n",
    "        \"a photo of car\",\n",
    "        \"a photo of road\",\n",
    "        \"a photo of tree\"\n",
    "          ]]\n",
    "i = 0  # Retrieve predictions for the first image for the corresponding text queries\n",
    "text = texts[i]\n",
    "\n",
    "img_path = \"E:/Dataset/xxxx/test_img/1.326936_103.891537_50f562ddfdc9f065f0005b17_Singapore.JPG\"\n",
    "image = Image.open(img_path)\n",
    "\n",
    "# predictions(image, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 将19类别转化为one-hot编码\n",
    "def label_to_onehot(label_id, num_classes):\n",
    "    # 创建一个所有元素都是0的向量\n",
    "    one_hot = np.zeros(num_classes, dtype=np.float32)\n",
    "    one_hot[label_id] = 1.0\n",
    "    return one_hot\n",
    "\n",
    "def build_graph(boxes, labels, numclas):\n",
    "\n",
    "    # 创建图对象\n",
    "    graph = nx.Graph()\n",
    "\n",
    "    # 添加框作为节点\n",
    "    for i, (box, label) in enumerate(zip(boxes, labels)):\n",
    "        one_hot_label = label_to_onehot(label, num_classes=numclas)\n",
    "        graph.add_node(i, \n",
    "                       box=box,\n",
    "                       label_class=one_hot_label)\n",
    "\n",
    "    # 根据接触或包含关系添加边\n",
    "    for i in range(len(boxes)):\n",
    "        for j in range(i+1, len(boxes)):\n",
    "            if is_touching_or_contained(boxes[i], boxes[j]):\n",
    "                graph.add_edge(i, j)\n",
    "\n",
    "    return graph\n",
    "\n",
    "def is_touching_or_contained(box1, box2):\n",
    "    # 判断box1与box2是否有接触或包含关系\n",
    "    cx1, cy1, w1, h1 = box1\n",
    "    cx2, cy2, w2, h2 = box2\n",
    "    return abs(cx1 - cx2) <= (w1 + w2) / 2 and abs(cy1 - cy2) <= (h1 + h2) / 2\n",
    "\n",
    "# 调用函数构建图\n",
    "boxes_node, labels_node = predictions(image, text)\n",
    "graph = build_graph(boxes_node, labels_node, numclas=len(text))\n",
    "\n",
    "# 绘制图\n",
    "plt.figure(figsize=(8, 8))\n",
    "pos = nx.spring_layout(graph, seed=42)\n",
    "\n",
    "nx.draw_networkx(graph, pos, node_color='red', node_size=200, font_size=8, with_labels=False)\n",
    "nx.draw_networkx_labels(graph, pos, font_color='white', font_size=8)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将类别转化为one-hot编码\n",
    "def label_to_onehot(label_id, num_classes):\n",
    "    # 创建一个所有元素都是0的向量\n",
    "    one_hot = np.zeros(num_classes, dtype=np.float32)\n",
    "    one_hot[label_id] = 1.0\n",
    "    return one_hot\n",
    "\n",
    "def create_graph(image_path, text):\n",
    "    image = Image.open(image_path)\n",
    "    boxes, labels = predictions(image, text)\n",
    "    # 创建图对象\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # 添加框作为节点\n",
    "    for i, (box, label) in enumerate(zip(boxes, labels)):\n",
    "        one_hot_label = label_to_onehot(label, num_classes=len(text))\n",
    "        G.add_node(i, \n",
    "                       box=box,\n",
    "                       label_class=one_hot_label)\n",
    "\n",
    "    # 根据接触或包含关系添加边\n",
    "    for i in range(len(boxes)):\n",
    "        for j in range(i+1, len(boxes)):\n",
    "            if is_touching_or_contained(boxes[i], boxes[j]):\n",
    "                G.add_edge(i, j)\n",
    "\n",
    "    return G\n",
    "\n",
    "def is_touching_or_contained(box1, box2):\n",
    "    # 判断box1与box2是否有接触或包含关系\n",
    "    cx1, cy1, w1, h1 = box1\n",
    "    cx2, cy2, w2, h2 = box2\n",
    "    return abs(cx1 - cx2) <= (w1 + w2) / 2 and abs(cy1 - cy2) <= (h1 + h2) / 2\n",
    "\n",
    "# 将图转化为dgl格式\n",
    "def networkx_to_dgl(G):\n",
    "    G = G.to_undirected()\n",
    "    g = dgl.from_networkx(G)\n",
    "\n",
    "    for node in G.nodes():\n",
    "        g.nodes[node].data['features'] = torch.cat(\n",
    "            [torch.tensor([G.nodes[node]['label_class']], dtype=torch.float)], dim=1)\n",
    "    g = dgl.add_self_loop(g)  # 添加自循环，考虑到自身的特征\n",
    "    return g\n",
    "\n",
    "class ImageGraphDataset(DGLDataset):\n",
    "    def __init__(self, image_paths, labels, texts, raw_dir=None, force_reload=False, verbose=False):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.texts = texts\n",
    "        super(ImageGraphDataset, self).__init__(name='ImageGraph',\n",
    "                                               raw_dir=raw_dir,\n",
    "                                               force_reload=force_reload,\n",
    "                                               verbose=verbose)\n",
    "\n",
    "    def process(self):\n",
    "        if not os.path.exists(self.raw_dir):\n",
    "            os.makedirs(self.raw_dir)\n",
    "\n",
    "        graph_path = os.path.join(self.raw_dir, 'place_graphs.dat') # 定义保存的名称\n",
    "        label_path = os.path.join(self.raw_dir, 'place_labels.dat')\n",
    "\n",
    "        if os.path.exists(graph_path) and os.path.exists(label_path):\n",
    "            with open(graph_path, 'rb') as f:\n",
    "                self.graphs = pickle.load(f)\n",
    "\n",
    "            with open(label_path, 'rb') as f:\n",
    "                self.processed_labels = pickle.load(f)\n",
    "\n",
    "        else:\n",
    "            numbbb = len(self.image_paths)\n",
    "            print(numbbb)\n",
    "\n",
    "            self.graphs = []\n",
    "            self.processed_labels = []\n",
    "\n",
    "            with tqdm(range(numbbb)) as pbar:\n",
    "                for i, image_path in enumerate(self.image_paths):\n",
    "                    G = create_graph(image_path, self.texts)\n",
    "\n",
    "                    if G.number_of_edges() > 0:\n",
    "                        g = networkx_to_dgl(G)\n",
    "                        self.graphs.append(g)\n",
    "\n",
    "                        label = torch.tensor(self.labels[i])\n",
    "                        self.processed_labels.append(label)\n",
    "\n",
    "                    pbar.update(1)\n",
    "\n",
    "            with open(graph_path, 'wb') as f:\n",
    "                pickle.dump(self.graphs, f)\n",
    "            \n",
    "            with open(label_path, 'wb') as f:\n",
    "                pickle.dump(self.processed_labels, f) \n",
    "\n",
    "    @property\n",
    "    def num_tasks(self):\n",
    "        return len(self.labels.unique())\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.graphs[i], self.processed_labels[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 24/51 [00:33<00:40,  1.50s/it]"
     ]
    }
   ],
   "source": [
    "# 处理数据，生成图分类数据集（dgl库）***\n",
    "\n",
    "texts = [[\n",
    "        # \"a photo of people\",\n",
    "        \"a photo of building\",\n",
    "        \"a photo of sky\",\n",
    "        \"a photo of car\",\n",
    "        \"a photo of road\",\n",
    "        # \"a photo of tree\"\n",
    "          ]]\n",
    "i = 0  # Retrieve predictions for the first image for the corresponding text queries\n",
    "text = texts[i]\n",
    "\n",
    "# proccess后的文件路径\n",
    "root_dir = 'E:/Dataset/xxxx/test'\n",
    "\n",
    "# 加载文件，包含path和label*\n",
    "img_name = pd.read_csv(\"E:/Dataset/xxxx/image_paths_placepulse_test.csv\")\n",
    "\n",
    "image_path = img_name['path']\n",
    "labels = img_name['label']\n",
    "\n",
    "dataset = ImageGraphDataset(raw_dir=root_dir, \n",
    "                            image_paths=image_path, \n",
    "                            labels=labels,\n",
    "                            texts=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_16248\\328130788.py:42: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  [torch.tensor([G.nodes[node]['label_class']], dtype=torch.float)], dim=1)\n"
     ]
    }
   ],
   "source": [
    "texts = [[\n",
    "        \"a photo of people\",\n",
    "        \"a photo of building\",\n",
    "        \"a photo of sky\",\n",
    "        \"a photo of car\",\n",
    "        \"a photo of road\",\n",
    "        \"a photo of tree\"\n",
    "          ]]\n",
    "i = 0  # Retrieve predictions for the first image for the corresponding text queries\n",
    "text = texts[i]\n",
    "\n",
    "img_path = \"E:/Dataset/xxxx/test_img/1.326936_103.891537_50f562ddfdc9f065f0005b17_Singapore.JPG\"\n",
    "\n",
    "G = networkx_to_dgl(create_graph(img_path, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=16, num_edges=108,\n",
       "      ndata_schemes={'features': Scheme(shape=(6,), dtype=torch.float32)}\n",
       "      edata_schemes={})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看生成的图数据集\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of classes: {dataset.num_tasks}')\n",
    "\n",
    "data_first = dataset[0][0]\n",
    "print()\n",
    "print(data_first)\n",
    "print('=============================================================')\n",
    "print(f'Number of nodes: {data_first.num_nodes()}')\n",
    "print(f'Number of edges: {data_first.num_edges()}')\n",
    "print(f'Average node degree: {data_first.num_edges() / data_first.num_nodes():.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
